{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1xSZ8EjHXBTVzSzpELDRSqn9k3haTFxYj","authorship_tag":"ABX9TyMObAcNuuaOD5zxrmecLzB4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jianlins/BMI_NLP_2024/blob/main/Module%205%20text%20classification%20demo.ipynb)\n","\n","# Sentence classification\n","\n","We will use previous [UUDeCART](https://github.com/UUDeCART/decart_rule_based_nlp) dataset. This dataset was created using the MIMIC demo dataset and was labeled by Dr. Barbara E. Jones. It is relatively small and was not annotated by a second annotator. Therefore, it should only be used for learning or demonstration purposes.\n","\n","We will start from a very simple implementation, just to get you familiar with a ML model training and evaluation process. And then you will try some extra exercises to see how you can make the baseline better."],"metadata":{"id":"8JcZOhxh0Rn4"}},{"cell_type":"markdown","source":["## Download the dataset"],"metadata":{"id":"eL5y70aI1W-j"}},{"cell_type":"code","source":["!wget https://github.com/UUDeCART/decart_rule_based_nlp/raw/master/data/training_v2.zip"],"metadata":{"id":"u-bUiTxtqluj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://github.com/UUDeCART/decart_rule_based_nlp/raw/master/data/test_v2.zip"],"metadata":{"id":"Efix7RESqo7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPNQmUpmqtrq","executionInfo":{"status":"ok","timestamp":1706807722002,"user_tz":420,"elapsed":151,"user":{"displayName":"Jade Broken","userId":"16103365057160619176"}},"outputId":"d7a9023b-5ac7-429b-90f7-2a40d284ac13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data  test_v2.zip  training_v2.zip\n"]}]},{"cell_type":"code","source":["!unzip training_v2.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFVFTfI_qvxp","executionInfo":{"status":"ok","timestamp":1706807766485,"user_tz":420,"elapsed":281,"user":{"displayName":"Jade Broken","userId":"16103365057160619176"}},"outputId":"f4beb296-c109-41fa-f98a-6715404d0145"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  training_v2.zip\n","caution: filename not matched:  test_v2.zip\n"]}]},{"cell_type":"code","source":["!unzip test_v2.zip"],"metadata":{"id":"yUc-FWcPqwi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0daHJoaq4MD","executionInfo":{"status":"ok","timestamp":1706807777517,"user_tz":420,"elapsed":158,"user":{"displayName":"Jade Broken","userId":"16103365057160619176"}},"outputId":"dab65fd4-950a-40e4-a8cc-2332aab440e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data  test_v2  test_v2.zip  training_v2\ttraining_v2.zip\n"]}]},{"cell_type":"markdown","source":["## Install & import the packages"],"metadata":{"id":"Q1FzWMzA1c1j"}},{"cell_type":"code","source":["!pip install quicksectx git+https://github.com/medspacy/medspacy_io"],"metadata":{"id":"ONbAsYwBq9bP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from spacy.lang.en import English\n","from medspacy_io.reader import BratDocReader\n","from medspacy_io.reader import BratDirReader\n","import spacy\n","from pathlib import Path\n","from medspacy_io.vectorizer import Vectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report"],"metadata":{"id":"bOTovA88rIrq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gGKInWvJrUPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The dataset files does not include schema configuration, let's create one\n","concepts=['EVIDENCE_OF_PNEUMONIA', 'PNEUMONIA_DOC_NO', 'PNEUMONIA_DOC_YES']\n","lines=['[entities]']+concepts\n","Path('annotation.conf').write_text('\\n'.join(lines))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nx6cwF9ZsQaq","executionInfo":{"status":"ok","timestamp":1706810622491,"user_tz":420,"elapsed":123,"user":{"displayName":"Jade Broken","userId":"16103365057160619176"}},"outputId":"d27dd542-70bd-4e7c-f112-24133f653882"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["67"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["# set up the Brat reader\n","nlp=spacy.load(\"en_core_web_sm\", disable=['ner'])\n","dir_reader = BratDirReader(nlp=nlp, support_overlap=True, recursive=True, schema_file='annotation.conf')"],"metadata":{"id":"rdi6uXEWrwxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This function will read brat annotation files and convert the snippet annotation into sentence labelled dataframe\n","def convert2df(data_folder):\n","  # read brat annotation into spaCy doc object.\n","  docs = dir_reader.read(txt_dir=data_folder)\n","  # convert snippet label into sentence-level labels and generate pandas dataframe\n","  df = Vectorizer.docs_to_sents_df(docs, track_doc_name=True)\n","  # remove document-level labels\n","  df=df[~df['y'].str.contains('_DOC_')]\n","  return df[['X','y']]\n","\n"],"metadata":{"id":"7GTpihbUyrRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df=convert2df('training_v2')"],"metadata":{"id":"IpYIjIwItMW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check the EVIDENCE_OF_PNEUMONIA annotations\n","train_df[train_df['y']!='NEG']"],"metadata":{"id":"T7JmmMO0tW1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take a look at https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","# and see what you can configure for this vectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(train_df['X'])"],"metadata":{"id":"Fk0j66ztxnJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# now we start to train a svm model\n","from sklearn.svm import SVC\n","model = SVC()\n","model.fit(X, train['y'])"],"metadata":{"id":"PwtcLWSVx4e7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"B33Acv9U3ufS"}},{"cell_type":"code","source":["# let's how it does on training set, this comparison usually is not considered as evaluation.\n","# But it can give us an impression about if the model complexity is sufficient, whether the model is overfitting, etc.\n","predictions = model.predict(X)"],"metadata":{"id":"d-QiZLGYyL3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(train_df['y'], predictions))"],"metadata":{"id":"8aTzdAiZycaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we take a look at test set."],"metadata":{"id":"V0V--RdL4bJH"}},{"cell_type":"code","source":["test_df=convert2df('test_v2')"],"metadata":{"id":"lPGb0MQ8ygof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: you will have to use \"transform\" here, instead of \"fit_transform\", why?\n","X_test = vectorizer.transform(test_df['X'])"],"metadata":{"id":"4mF6kGEczvxG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_preds=model.predict(X_test)"],"metadata":{"id":"PqptQKa5z2W3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(test_df['y'], y_test_preds))"],"metadata":{"id":"iiEdPUJKz5wJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Compare** the performance above, what the difference tells you?\n","\n","Now we take a closer look to the errors"],"metadata":{"id":"o3iu9xvq4tOU"}},{"cell_type":"code","source":["import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n"],"metadata":{"id":"alXPyCAt5W3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df['pred']=y_test_preds"],"metadata":{"id":"tVA0Ui7Z0Fmq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df[test_df['y']!=test_df['pred']][:5]"],"metadata":{"id":"2yYtzoIr5HQb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check a few more errors.\n","What have you found? What's the possible cause of these errors?\n","\n","\n","## Now let's try applying TfIdf\n","Read this page and examples\n","https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n","\n","## Exercise:\n","* Implement your solution.\n","* Try at least 3 more tricks that you think would be effective and see if these methods can help improve the performance, e.g. stemming, normalization, etc.\n","* Instead of perform sentence classification, try document classification instead. (Hint: Inside the function \"convert2df\", we filtered out the document level annotations. For this task, you will actually use these labels and disregard 'EVIDENCE_OF_PNEUMONIA')\n","\n","\n","You will be asked to demonstrate and explain your work during the class.\n"],"metadata":{"id":"nPZBFCtM52Fv"}}]}